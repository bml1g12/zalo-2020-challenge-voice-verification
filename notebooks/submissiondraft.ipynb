{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ben/datadrive/Software/pyannote-audio/data/ami/voxceleb_finetuneexp2_loose/config.yml\n",
      "CONFIG {'duration': 4.0, 'label_min_duration': 30, 'margin': 0.05, 'min_duration': 1.0, 'per_fold': 128, 'per_label': 1, 'per_turn': 1, 's': 10}\n",
      "Loading weight /media/ben/datadrive/Software/pyannote-audio/data/ami/voxceleb_finetuneexp2_loose/train/ZALODATASET.SpeakerVerification.BenProtocol.train/weights/0049.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ben/datadrive/Software/pyannote-audio/pyannote/audio/embedding/approaches/arcface_loss.py:170: FutureWarning: The 's' parameter is deprecated in favor of 'scale', and will be removed in a future release\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "  0%|          | 11/50000 [00:15<18:59:54,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric       0.99978\n",
      "threshold    0.99900\n",
      "ratio        0.00022\n",
      "Name: 731, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I set the cropping to \"loose\" in pyannote.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pyannote.audio.features import Pretrained\n",
    "from pyannote.core.utils.distance import l2_normalize\n",
    "from pyannote.core.utils.distance import cdist\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "threshold = 0.792 # from val set\n",
    "expt_name = \"embeddings_voxceleb_49epochfinetuned\"\n",
    "emb = Pretrained(validate_dir='/media/ben/datadrive/Software/pyannote-audio/data/ami/voxceleb_finetuneexp2_loose/train/ZALODATASET.SpeakerVerification.BenProtocol.train/validate_equal_error_rate/ZALODATASET.SpeakerVerification.BenProtocol.development/',\n",
    "                 epoch=49,\n",
    "                 device=\"cpu\")\n",
    "\n",
    "\n",
    "os.environ[\"PYANNOTE_DATABASE_CONFIG\"] = \"/media/ben/datadrive/Software/pyannote-audio/data/ami/\"\n",
    "# speaker embedding model trained on AMI training set\n",
    "# emb = torch.hub.load('pyannote/pyannote-audio', 'emb_voxceleb')\n",
    "expt_root = \"/media/ben/datadrive/Zalo/voice-verification/\"\n",
    "dataset_path = os.path.abspath(os.path.join(expt_root, \"Train-Test-Data/public-test.csv\"))\n",
    "df_test_sub = pd.read_csv(dataset_path)\n",
    "\n",
    "filename2embedding = {}\n",
    "#with open(f\"{expt_name}embedding_public.pickle\", \"rb\") as input_file:\n",
    "#    filename2embedding = pickle.load(input_file)\n",
    "counter = 0\n",
    "with tqdm(total=len(df_test_sub)) as pbar:\n",
    "    for i, row in df_test_sub.iterrows():\n",
    "        audio1_filepath = os.path.join(expt_root, \"Train-Test-Data/public-test/\") + str(row[\"audio_1\"]) \n",
    "        if audio1_filepath in filename2embedding:\n",
    "            audio1_embedding = filename2embedding[audio1_filepath]\n",
    "        else:\n",
    "            audio1_embedding = np.mean(emb(\n",
    "                {\"uri\": row[\"audio_1\"], \"audio\": audio1_filepath}),\n",
    "                                       axis=0, keepdims=True)\n",
    "            filename2embedding[audio1_filepath] = audio1_embedding\n",
    "        \n",
    "        audio2_filepath = os.path.join(expt_root, \"Train-Test-Data/public-test/\") + str(row[\"audio_2\"])    \n",
    "        if audio2_filepath in filename2embedding:\n",
    "            audio2_embedding = filename2embedding[audio2_filepath]\n",
    "        else:\n",
    "            audio2_embedding = np.mean(emb(\n",
    "            {\"uri\": row[\"audio_2\"], \"audio\": audio2_filepath}),\n",
    "                                   axis=0, keepdims=True)\n",
    "            filename2embedding[audio2_filepath] = audio2_embedding\n",
    "        \n",
    "        # X_audio1 = l2_normalize(np.array([audio1_embedding,]))\n",
    "        # X_audio2 = l2_normalize(np.array([audio2_embedding,]))\n",
    "        distance = cdist(audio1_embedding, audio2_embedding, metric=\"cosine\")\n",
    "        #if (i % 1000) == 0:\n",
    "        #    print(f\"Distance is {distance[0][0]} for index {i} \")\n",
    "        dist = distance[0][0]\n",
    "        df_test_sub.loc[i, \"dist\"] = dist\n",
    "        df_test_sub.loc[i, \"audio1_filepath\"] = audio1_filepath\n",
    "        df_test_sub.loc[i, \"audio2_filepath\"] = audio2_filepath\n",
    "\n",
    "\n",
    "        pbar.update()\n",
    "        counter+=1\n",
    "        if  counter > 10:\n",
    "            break\n",
    "        \n",
    "with open(f\"{expt_name}embedding_public.pickle\", 'wb') as handle:\n",
    "    pickle.dump(filename2embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#def get_duration(path):\n",
    "#    duration = librosa.get_duration(filename=path)\n",
    "#    return duration\n",
    "#df_test_sub.loc[:, \"audio1_duration\"] = df_test_sub[\"audio1_filepath\"].apply(get_duration)\n",
    "#df_test_sub.loc[:, \"audio2_duration\"] = df_test_sub[\"audio2_filepath\"].apply(get_duration)\n",
    "#df_test_sub[\"duration_dif\"] = abs(df_test_sub[\"audio1_duration\"] - df_test_sub[\"audio2_duration\"])\n",
    "        \n",
    "def get_optimal_balanced_threshold(df):\n",
    "    rows = []\n",
    "    for i in np.arange(0, 1, 0.001):\n",
    "        result = (df[\"dist\"] < i).value_counts()\n",
    "        if len(result) == 1:\n",
    "            # If only True or False then its a bad threshold and move on\n",
    "            continue\n",
    "        ratio = result.loc[True] / result.loc[False]\n",
    "        minimise_metric = abs((ratio) -1)\n",
    "        rows.append({\"metric\": minimise_metric, \"threshold\":i, \"ratio\": ratio})\n",
    "    _ = pd.DataFrame(rows)\n",
    "    return _.sort_values(\"metric\").iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "result = get_optimal_balanced_threshold(df_test_sub)\n",
    "df_test_sub[\"label\"] = (df_test_sub[\"dist\"] < result[\"threshold\"]).astype(int)\n",
    "\n",
    "\n",
    "print(result)\n",
    "df_write = df_test_sub[[\"audio_1\", \"audio_2\", \"label\"]]\n",
    "df_write.to_csv(f\"{expt_name}_threshold{result['threshold']}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyannote",
   "language": "python",
   "name": "pyannote"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
